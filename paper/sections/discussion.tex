%% ============================================================================
%% DISCUSSION
%% ============================================================================

\section{Discussion}

\subsection{Why Does Pressure-Field Dominate?}

Our results contradict the intuition that explicit coordination should outperform implicit coordination. We identify three factors explaining pressure-field's dominance:

\textbf{Coordination overhead harms performance.} Hierarchical systems spend computational budget on coordination rather than problem-solving. The manager-worker protocol requires multiple \ac{llm} calls per patch (planning, delegation, execution), while pressure-field requires only one (patch proposal). This overhead compounds: hierarchical attempts fewer patches per tick, reducing exploration.

\textbf{Local greedy decisions are effective for constraint satisfaction.} Meeting room scheduling exhibits locality: fixing a conflict in one time block rarely creates conflicts in distant blocks. This matches pressure-field's locality assumption, making greedy local optimization effective. Hierarchical coordination's global planning provides no benefit for locally-decomposable problems.

\textbf{Parallel validation amplifies pressure-field's advantage.} Pressure-field validates patches for multiple regions simultaneously, applying the highest-scoring patch per region that reduces pressure. Hierarchical validates one patch at a time, requiring multiple ticks to explore alternatives. On problems with many valid solutions, parallel exploration finds solutions faster.

\subsection{Failure Analysis: Why Hierarchical Collapses}

The 30$\times$ performance gap (48.5\% vs 1.5\%) demands deeper investigation. Analysis of hierarchical trials reveals a catastrophic failure pattern: the \emph{rejection loop}.

\textbf{The rejection loop mechanism.} Hierarchical always selects the highest-pressure region for improvement. But the highest-pressure regions are high-pressure precisely because they are difficult to improve. When the \ac{llm} proposes a patch that fails validation (does not reduce pressure), the region remains highest-pressure and is selected again next tick. This creates a self-reinforcing cycle: 66.7\% of hierarchical runs (180/270) applied zero patches across all 50 ticks, stuck targeting the same intractable region repeatedly.

\textbf{Rejection rates confirm the pattern.} Across all hierarchical trials, only 173 patches were accepted out of 13,460 proposed---a 98.7\% rejection rate. By contrast, pressure-field's parallel exploration means that even if one agent's patch is rejected, other agents make progress on different regions. The non-blocking architecture prevents any single difficult region from stalling the entire system.

\textbf{The architectural lesson.} Hierarchical's design embodies a reasonable intuition: focus intelligent effort on the worst problems. But this creates a trap when combined with strict validation: the hardest problems resist improvement, causing repeated rejection, which blocks progress everywhere. Pressure-field avoids this trap through distributed exploration---progress happens where it can, not where a central planner dictates it must.

\subsection{Limitations}

Our experiments reveal several important limitations:

\textbf{Absolute solve rates are modest on hard problems.} Even pressure-field achieves only 15.6\% on hard problems. Meeting room scheduling with tight constraints (5 rooms, 60 meetings, 30\% pre-scheduled) remains challenging for small models (0.5b--3b parameters); larger models may achieve higher absolute solve rates.

\textbf{Domain specificity.} Results on meeting room scheduling may not generalize to domains lacking measurable pressure gradients or locality properties. Tasks requiring global planning or long-horizon reasoning may favor hierarchical approaches.

\textbf{Additional practical limitations:}
\begin{itemize}
\item Requires well-designed pressure functions (not learned from data)
\item Decay rates $\lambda_f, \lambda_\gamma$ and inhibition period require task-specific tuning
\item May not suit tasks requiring long-horizon global planning
\item Goodhart's Law: agents may game poorly-designed metrics
\item Resource cost of parallel validation: testing $K$ patches requires $O(K \cdot |A|)$ memory where $|A|$ is artifact size
\end{itemize}

\subsection{When to Choose Each Approach}

Our results suggest the following guidance:

\textbf{Pressure-field coordination is preferable when:}
\begin{enumerate}
\item \textbf{Performance matters.} Pressure-field achieves 3--30$\times$ higher solve rates than alternatives.
\item \textbf{Simplicity is valued.} No coordinator agent needed; coordination emerges from shared state.
\item \textbf{Fault tolerance matters.} No single point of failure; agents can join/leave without protocol overhead.
\item \textbf{Pressure signals are available.} The domain provides measurable quality gradients.
\item \textbf{Problems are locally decomposable.} Local fixes improve global quality without cascading conflicts.
\end{enumerate}

\textbf{Hierarchical coordination may be appropriate when:}
\begin{enumerate}
\item \textbf{Explicit control is required.} Some domains require deterministic task assignment for regulatory or safety reasons.
\item \textbf{Interpretability is critical.} Hierarchical task assignment provides clear audit trails.
\item \textbf{Global planning is essential.} Tasks with strong non-local dependencies may benefit from centralized reasoning.
\end{enumerate}

\subsection{Band and Model Escalation as Adaptive Capability}

All experiments use a two-level escalation mechanism. \emph{Band escalation} cycles through sampling strategies (Exploitation $\to$ Balanced $\to$ Exploration, 7 ticks each) before \emph{model escalation} progresses through model sizes (0.5b $\to$ 1.5b $\to$ 3b parameters). Model escalation triggers when regions remain high-pressure for 21 consecutive ticks.

This mechanism proves beneficial for pressure-field: on hard problems, pressure-field achieves 15.6\% with escalation enabled. The escalation mechanism works because larger models have broader solution coverage and different sampling bands explore different regions of solution space.

\subsection{Future Work}

\begin{itemize}
\item \textbf{Learned pressure functions}: Current sensors are hand-designed. Can we learn pressure functions from solution traces?
\item \textbf{Adversarial robustness}: Can malicious agents exploit pressure gradients to degrade system performance?
\item \textbf{Multi-artifact coordination}: Extension to coupled artifacts where patches in one affect pressure in another
\item \textbf{Larger-scale experiments}: Testing on schedules with more rooms and longer time horizons to characterize scaling limits
\item \textbf{Alternative domains}: Applying pressure-field coordination to code refactoring, configuration management, and other artifact refinement tasks
\end{itemize}

\subsection{Societal Implications}

Pressure-field coordination raises societal concerns that extend beyond technical performance. We identify three critical issues---accountability attribution, metric gaming through Goodhart's Law, and explainability challenges---that require deliberate design choices in deployment.

\subsubsection{Accountability and Attribution}

When coordination emerges from shared pressure gradients rather than explicit delegation, attributing outcomes to individual agents becomes challenging. In hierarchical systems, task assignment creates clear accountability chains. In pressure-field coordination, multiple agents may contribute to a region through independent pressure-reducing actions, with no record of which agent ``owned'' the outcome.

This accountability diffusion has both benefits and risks. The benefit is fault tolerance: agent failures degrade performance gracefully rather than catastrophically. The risk is opacity in failure analysis: identifying which agent proposed a problematic patch---and what pressure signal motivated it---requires detailed logging that the minimal coordination mechanism does not inherently provide.

For deployment in regulated domains, this suggests an augmentation requirement: pressure-field systems must maintain audit logs recording patch provenance, pressure signals at proposal time, and validation outcomes. The coordination mechanism remains simple---agents coordinate through shared state---but operational deployment adds logging infrastructure preserving accountability.

\subsubsection{Goodhart's Law and Metric Gaming}

Goodhart's Law states: ``When a measure becomes a target, it ceases to be a good measure.'' Pressure-field coordination is vulnerable to this dynamic because agents are optimized to reduce pressure as defined by designer-specified functions. If those functions imperfectly capture true quality---and they inevitably do---agents will discover and exploit the mismatch.

Consider code quality pressure functions penalizing complexity metrics. An agent might reduce complexity by splitting functions excessively, harming readability while improving the metric. The mitigation is not abandoning pressure functions but designing them defensively: use multiple orthogonal pressure axes, include adversarial sensors detecting gaming strategies, and audit whether pressure reduction correlates with human quality judgments. Pressure functions should evolve as agents discover exploits.

Foundation models introduce second-order gaming concerns: \acp{llm} trained on internet-scale text may have implicit knowledge of how to game specific benchmarks. This suggests pressure functions for \ac{llm}-based systems should favor domain-specific quality signals harder to optimize without genuine improvement.

\subsubsection{Explainability Challenges}

In hierarchical systems, explanations follow delegation chains: ``Manager X assigned task Y to Worker Z because condition C held.'' In pressure-field coordination, the explanation is: ``Region R had high pressure, agent A proposed patch $\Delta$ reducing pressure by $\delta$.'' This is mechanistically transparent but causally opaque---it describes what happened without explaining why that particular patch was chosen.

This is the explainability trade-off inherent to emergent coordination: simplicity in mechanism comes at the cost of legibility in rationale. For many domains---code formatting, resource optimization, routine maintenance---the trade-off is acceptable: outcomes are verifiable even if reasoning is opaque. For high-stakes domains requiring human oversight, opacity is unacceptable.

The design implication is domain-dependent deployment: pressure-field coordination suits domains where outcome verification is cheap even if reasoning transparency is limited. For domains requiring justification to human stakeholders, hierarchical coordination remains necessary despite overhead costs.

\subsubsection{Design Implications}

These concerns suggest three requirements for responsible deployment: comprehensive audit logging preserving patch provenance and pressure signals, defensive pressure function design with multiple orthogonal axes, and domain-appropriate verification matching coordination opacity with outcome verifiability. The coordination mechanism remains simple---but responsible deployment requires surrounding infrastructure addressing accountability, gaming, and explainability.
