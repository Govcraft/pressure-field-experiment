%% ============================================================================
%% INTRODUCTION
%% ============================================================================

\section{Introduction}

Multi-agent systems built on large language models address complex task automation~\cite{wu2023autogen,hong2023metagpt,li2023camel}. The dominant paradigm treats agents as organizational units: planners decompose tasks, managers delegate subtasks, and workers execute instructions under hierarchical supervision. This coordination overhead scales poorly with agent count and task complexity.

We demonstrate that \emph{implicit} coordination through shared state outperforms explicit hierarchical control---without coordinators, planners, or message passing. Across 1350 total trials on meeting room scheduling (270 per strategy), pressure-field coordination achieves over 30$\times$ higher solve rates than hierarchical control and 4$\times$ higher than conversation-based approaches~\cite{wu2023autogen} (all $p < 0.001$ with large effect sizes). Sequential and random baselines achieve only 0.4\%.

Our approach draws inspiration from natural coordination mechanisms---ant colonies, immune systems, neural tissue---that coordinate through \emph{environment modification} rather than message passing. Agents observe local quality signals (pressure gradients), take locally-greedy actions, and coordination emerges from shared artifact state. The key insight is that \emph{local greedy decisions are effective for constraint satisfaction}: when problems exhibit locality (fixing one region rarely breaks distant regions), decentralized greedy optimization outperforms centralized planning. Temporal decay prevents premature convergence by ensuring continued exploration.

Our contributions:

\begin{enumerate}
\item We formalize \emph{pressure-field coordination} as a role-free, stigmergic alternative to organizational \ac{mas} paradigms. Unlike \ac{gpgp}'s hierarchical message-passing or SharedPlans' intention alignment, pressure-field achieves $O(1)$ coordination overhead through shared artifact state. Foundation models enable this approach: their broad pretraining allows quality-improving patches from local pressure signals without domain-specific coordination protocols.

\item We introduce \emph{temporal decay} as a mechanism for preventing premature convergence. Ablation studies show a 10 percentage point improvement with decay enabled (96.7\% vs 86.7\%), directionally consistent with the theoretical prediction that decay helps escape local minima, though not statistically significant at $n=30$.

\item We prove convergence guarantees for this coordination scheme under pressure alignment conditions.

\item We provide empirical evidence across 1350 total trials (270 per strategy) showing: (a) pressure-field dramatically outperforms all baselines by an order of magnitude or more, (b) all pairwise comparisons are highly significant ($p < 0.001$).
\end{enumerate}

This work demonstrates that \ac{fm} capabilities and \ac{mas} coordination mechanisms are \emph{mutually enabling}, not merely additive. \Acp{fm} solve a fundamental \ac{mas} problem: traditional coordination requires explicit action space enumeration, but for open-ended artifact refinement the space of valid improvements is unbounded. \Acp{fm}' broad pretraining provides implicit coverage of improvement strategies without domain-specific action representations---a ``universal actor'' capability. Conversely, \ac{mas} coordination solves a fundamental \ac{fm} problem: how to combine multiple model outputs coherently. Pressure gradients provide an objective criterion for output selection, replacing ad-hoc voting or ranking with principled quality-based filtering. This bidirectional synthesis explains why pressure-field coordination outperforms conversation-based alternatives that lack objective gradients for output combination.
