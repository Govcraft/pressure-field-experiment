\begin{abstract}
Current multi-agent \ac{llm} frameworks rely on explicit orchestration patterns borrowed from human organizational structures: planners delegate to executors, managers coordinate workers, and hierarchical control flow governs agent interactions. These approaches suffer from coordination overhead that scales poorly with agent count and task complexity. We propose a fundamentally different paradigm inspired by natural coordination mechanisms: agents operate locally on a shared artifact, guided only by pressure gradients derived from measurable quality signals, with temporal decay preventing premature convergence. We formalize this as optimization over a pressure landscape and prove convergence guarantees under mild conditions.

Empirically, on meeting room scheduling across 1350 trials, pressure-field coordination outperforms all baselines: 48.5\% aggregate solve rate across all difficulty levels (86.7\% on easy problems) versus 12.6\% for conversation-based coordination, 1.5\% for hierarchical control, and 0.4\% for sequential and random baselines (all pairwise comparisons $p < 0.001$). Temporal decay is essential: disabling it reduces solve rate by 10 percentage points. On easy problems, pressure-field achieves 86.7\% solve rate compared to 37.8\% for the next-best baseline. The approach maintains consistent performance from 1 to 4 agents. Implicit coordination through shared pressure gradients outperforms explicit hierarchical control. Foundation models enable this approach: their broad pretraining and zero-shot reasoning allow quality-improving patches from local pressure signals alone, without domain-specific coordination protocols. This suggests that constraint-driven emergence offers a simpler and more effective foundation for multi-agent AI.
\end{abstract}

\begin{center}
\begin{minipage}{0.85\textwidth}
\small\textbf{Keywords:} multi-agent systems, emergent coordination, decentralized optimization, \acs{llm} agents
\end{minipage}
\end{center}
\vspace{1em}
