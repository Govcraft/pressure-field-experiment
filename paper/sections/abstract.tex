\begin{abstract}
Current multi-agent \ac{llm} frameworks rely on explicit orchestration patterns borrowed from human organizational structures: planners delegate to executors, managers coordinate workers, and hierarchical control flow governs agent interactions. These approaches suffer from coordination overhead that scales poorly with agent count and task complexity. We propose a fundamentally different paradigm inspired by natural coordination mechanisms: agents operate locally on a shared artifact, guided only by pressure gradients derived from measurable quality signals, with temporal decay preventing premature convergence. We formalize this as optimization over a pressure landscape and prove convergence guarantees under mild conditions.

Empirically, on meeting room scheduling across 1350 total trials (270 per strategy), pressure-field coordination achieves $4\times$ higher solve rates than conversation-based coordination~\cite{wu2023autogen} and over $30\times$ higher than hierarchical control (48.5\% vs 11.1\% vs 1.5\%; all pairwise comparisons $p < 0.001$). Ablation studies suggest temporal decay is beneficial, with a 10 percentage point improvement observed though not statistically significant at $n=30$. On easy problems, pressure-field achieves 86.7\% solve rate compared to 33.3\% for the next-best baseline. The approach maintains consistent performance from 1 to 4 agents. Implicit coordination through shared pressure gradients outperforms explicit hierarchical control. Foundation models enable this approach: their broad pretraining and zero-shot reasoning allow quality-improving patches from local pressure signals alone, without domain-specific coordination protocols. This suggests that constraint-driven emergence offers a simpler and more effective foundation for multi-agent AI.
\end{abstract}

\begin{center}
\begin{minipage}{0.85\textwidth}
\small\textbf{Keywords:} multi-agent systems, emergent coordination, decentralized optimization, \acs{llm} agents
\end{minipage}
\end{center}
\vspace{1em}
