%% ============================================================================
%% EXPERIMENTS
%% ============================================================================

\section{Experiments}

We evaluate pressure-field coordination on meeting room scheduling: assigning $N$ meetings to $R$ rooms over $D$ days to minimize gaps (unscheduled time), overlaps (attendee double-bookings), and maximize utilization balance. This domain provides continuous pressure gradients (rather than discrete violations), measurable success criteria, and scalable difficulty through problem size.

\textbf{Key findings}: Pressure-field coordination outperforms all baselines (\S\ref{sec:main-results}). Temporal decay shows a beneficial trend, though statistical significance requires larger samples (\S\ref{sec:ablations}). The approach maintains consistent performance from 1 to 4 agents (\S\ref{sec:scaling}). Despite using more tokens per trial, pressure-field achieves 12\% better token efficiency per successful solve (\S\ref{sec:token-efficiency}).

\subsection{Setup}

\subsubsection{Task: Meeting Room Scheduling}

We generate scheduling problems with varying difficulty:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Difficulty} & \textbf{Rooms} & \textbf{Meetings} & \textbf{Pre-scheduled} \\
\midrule
Easy & 3 & 20 & 70\% \\
Medium & 5 & 40 & 50\% \\
Hard & 5 & 60 & 30\% \\
\bottomrule
\end{tabular}
\caption{Problem configurations. Pre-scheduled percentage indicates meetings already placed; remaining meetings must be scheduled by agents.}
\label{tab:problems}
\end{table}

Each schedule spans 5 days with 30-minute time slots (8am--4pm). Regions are 2-hour time blocks (4 blocks per day $\times$ 5 days = 20 regions per schedule). A problem is ``solved'' when all meetings are scheduled with zero attendee overlaps within 50 ticks.

\textbf{Pressure function}: $P = \text{gaps} \cdot 1.0 + \text{overlaps} \cdot 2.0 + \text{util\_var} \cdot 0.5 + \text{unsched} \cdot 1.5$

where $\text{gaps}$ measures empty slots as a fraction, $\text{overlaps}$ counts attendee double-bookings, $\text{util\_var}$ measures room utilization variance, and $\text{unsched}$ is the fraction of unscheduled meetings.

\textbf{Alignment verification}: The per-region pressure computation uses only $\text{gaps}$, $\text{overlaps}$, and $\text{util\_var}$---all strictly local to each time block. The $\text{unsched}$ component is added to total pressure only, not per-region. This makes the per-region pressure \emph{separable}: modifying region $i$ has zero effect on region $j$'s pressure for $j \neq i$, satisfying the alignment condition (Definition~2) with $\epsilon = 0$. While attendee constraints could theoretically create cross-region coupling (the same person attending meetings in different time blocks), our overlap sensor counts overlaps only within each time block, eliminating this coupling source. Empirical analysis (Appendix~B) confirms that all observed pressure improvements are positive, consistent with separable pressure.

\subsubsection{Baselines}

We compare five coordination strategies, all using identical \acp{llm} (\texttt{qwen2.5:0.5b/1.5b/3b} via Ollama) to isolate coordination effects:

\textbf{Pressure-field (ours)}: Full system with decay (fitness half-life 5s), inhibition (2s cooldown), greedy region selection (highest-pressure region per tick), and parallel validation. Includes band escalation (Exploitation $\to$ Balanced $\to$ Exploration) and model escalation (0.5b $\to$ 1.5b $\to$ 3b).

\textbf{Conversation}: AutoGen-style multi-agent dialogue where agents exchange messages to coordinate scheduling decisions. Agents discuss conflicts and propose solutions through explicit communication.

\textbf{Hierarchical}: Single agent selects the highest-pressure time block each tick, proposes a schedule change, and validates before applying (only accepts pressure-reducing patches). Uses identical prompts to pressure-field. The differences are: (1) greedy region selection always targets the hardest region, and (2) sequential execution processes one region per tick. This represents centralized, quality-gated control.

\textbf{Sequential}: Single agent iterates through time blocks in fixed order, proposing schedule changes one region at a time. No parallelism, pressure guidance, or patch validation---applies any syntactically valid patch regardless of quality impact.

\textbf{Random}: Selects random time blocks and proposes schedule changes. No patch validation---applies any syntactically valid patch regardless of quality impact.

\textbf{Note on parallelism}: Pressure-field validates multiple patches in parallel ($K$ regions per tick), while hierarchical validates one patch sequentially. This asymmetry is \emph{inherent to the coordination paradigm}, not an implementation choice: hierarchical control requires the manager to select a region, delegate to a worker, and validate the result before proceeding---delegating to multiple workers simultaneously would require additional coordination protocols (work distribution, conflict resolution, result aggregation) that would transform it into a different architecture entirely. The sequential bottleneck is the cost of centralized control. When hierarchical's single patch is rejected, the tick produces no progress; when one of pressure-field's parallel patches is rejected, others may still succeed.

\textbf{Model choice rationale}: We deliberately use small, minimally-capable models (0.5b--3b parameters) rather than frontier models. This design choice strengthens our thesis: if the coordination mechanism can extract effective performance from weak models, the mechanism itself is valuable---independent of model capability. Using identical model chains across all strategies isolates coordination effects from model effects. We hypothesize that frontier models (e.g., GPT-4, Claude) would raise absolute solve rates across all strategies while preserving relative rankings: the coordination advantage is orthogonal to model capability, so pressure-field's $4\times$ improvement over conversation baselines should persist even as the baseline rises.

\subsubsection{Metrics}

\begin{itemize}
\item \textbf{Solve rate}: Percentage of schedules reaching all meetings placed with zero overlaps within 50 ticks.
\item \textbf{Ticks to solve}: Convergence speed for solved cases
\item \textbf{Final pressure}: Remaining gaps, overlaps, and unscheduled meetings for unsolved cases
\item \textbf{Token efficiency}: Total prompt and completion tokens consumed per trial and per successful solve
\end{itemize}

\subsubsection{Implementation}

\textbf{Hardware}: NVIDIA RTX 4070 8GB \ac{gpu}, AMD Ryzen 9 7940HS, 64GB RAM. \textbf{Software}: Rust implementation with Ollama. \textbf{Trials}: 30 per configuration. Full protocol in Appendix~A.

\textbf{Band escalation}: When pressure velocity (rate of improvement) drops to zero for 7 consecutive ticks, sampling parameters escalate: Exploitation (T=0.2, p=0.85) $\to$ Balanced (T=0.4, p=0.9) $\to$ Exploration (T=0.7, p=0.95).

\textbf{Model escalation}: After exhausting all bands with zero progress (21 ticks total), the system escalates through the model chain: 0.5b $\to$ 1.5b $\to$ 3b, resetting to Exploitation band. Section~\ref{sec:escalation} analyzes this mechanism.

\subsection{Main Results}
\label{sec:main-results}

Across 1350 total trials spanning three difficulty levels (easy, medium, hard) and agent counts (1, 2, 4), we find that pressure-field coordination outperforms all baselines:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Strategy} & \textbf{Solved/N} & \textbf{Rate} & \textbf{95\% Wilson \acs{ci}} \\
\midrule
Pressure-field & 131/270 & 48.5\% & 42.6\%--54.5\% \\
Conversation & 30/270 & 11.1\% & 7.9\%--15.4\% \\
Hierarchical & 4/270 & 1.5\% & 0.6\%--3.7\% \\
Sequential & 1/270 & 0.4\% & 0.1\%--2.1\% \\
Random & 1/270 & 0.4\% & 0.1\%--2.1\% \\
\bottomrule
\end{tabular}
\caption{Aggregate solve rates across all experiments (1350 total trials, 270 per strategy). Chi-square test across all five strategies: $\chi^2 > 200$, $p < 0.001$.}
\label{tab:main-results}
\end{table}

The results show clear stratification:

\textbf{Pressure-field dominates}: Pressure-field achieves 48.5\% solve rate, roughly 4$\times$ higher than the next-best baseline (conversation at 11.1\%). The effect size is large: Cohen's $h = 1.16$ versus conversation on easy problems, and $h > 1.97$ versus all other baselines.

\textbf{Conversation provides intermediate performance}: The AutoGen-style conversation baseline achieves 11.1\% overall, significantly better than hierarchical ($p < 0.001$) but far below pressure-field. Notably, conversation solves only easy problems (33.3\% on easy, 0\% on medium and hard).

\textbf{Hierarchical and sequential fail}: Despite explicit coordination, hierarchical control achieves only 1.5\% solve rate---comparable to random (0.4\%). Both strategies fail entirely on medium and hard problems.

This result contradicts the common assumption that explicit hierarchical coordination should outperform implicit coordination. The overhead of centralized control and message passing appears to harm rather than help performance on constraint satisfaction tasks.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{fig1_strategy_comparison.pdf}
\caption{Strategy comparison by difficulty level. Error bars show 95\% Wilson \acp{ci}. Pressure-field outperforms all baselines at every difficulty level. On medium and hard problems, only pressure-field achieves non-zero solve rates.}
\label{fig:strategy-comparison}
\end{figure}

\subsection{Ablations}
\label{sec:ablations}

\subsubsection{Effect of Temporal Decay}

Decay appears beneficial, though the effect does not reach statistical significance with our sample size:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Configuration} & \textbf{Solved/N} & \textbf{Solve Rate} & \textbf{95\% \acs{ci}} \\
\midrule
Full (with decay) & 29/30 & 96.7\% & 83.3\%--99.4\% \\
Without decay & 26/30 & 86.7\% & 70.3\%--94.7\% \\
\bottomrule
\end{tabular}
\caption{Decay ablation on easy scheduling problems (30 trials each). Fisher's exact test: $p = 0.35$, indicating the 10 percentage point difference is not statistically significant at $\alpha = 0.05$.}
\label{tab:decay-ablation}
\end{table}

The observed effect---a 10 percentage point reduction when decay is disabled---is directionally consistent with our theoretical predictions. The non-significant $p$-value ($p = 0.35$) reflects both limited sample size and a ceiling effect: the high baseline solve rate on easy problems (96.7\% with decay) leaves limited statistical room to detect improvement. The overlapping confidence intervals (83.3\%--99.4\% vs 70.3\%--94.7\%) reflect this uncertainty. Without decay, fitness saturates after initial patches---regions that received early patches retain high fitness indefinitely, making them appear ``stable'' even when they still contain unscheduled meetings. Since greedy selection prioritizes high-pressure regions, these prematurely-stabilized regions are never reconsidered. This mechanism is consistent with the Basin Separation result (Theorem~\ref{thm:basin-separation}): without decay, agents may remain trapped in the first stable basin they reach. Larger-scale ablation studies would be needed to establish the statistical significance of decay's contribution.

\subsubsection{Effect of Inhibition and Examples}

The ablation study tested combinations of decay, inhibition, and few-shot examples on easy scheduling problems:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Configuration} & \textbf{Decay} & \textbf{Inhib} & \textbf{Examples} & \textbf{Solve Rate} \\
\midrule
Full & \checkmark & \checkmark & \checkmark & 96.7\% \\
No Decay & $\times$ & \checkmark & \checkmark & 86.7\% \\
No Inhibition & \checkmark & $\times$ & \checkmark & 96.7\% \\
No Examples & \checkmark & \checkmark & $\times$ & 90.0\% \\
Baseline & $\times$ & $\times$ & $\times$ & 90.0\% \\
\bottomrule
\end{tabular}
\caption{Ablation results (30 trials each configuration on easy difficulty).}
\label{tab:ablation}
\end{table}

Feature contributions:
\begin{itemize}
\item \textbf{Decay}: +10.0\% (full 96.7\% vs no\_decay 86.7\%)
\item \textbf{Inhibition}: +0.0\% (no detectable effect)
\item \textbf{Examples}: +6.7\% (full 96.7\% vs no\_examples 90.0\%)
\end{itemize}

\emph{Decay shows the largest effect}: configurations with decay achieve higher solve rates, though the differences do not reach statistical significance at $n=30$. Inhibition shows no detectable effect in this domain, possibly because the 50-tick budget provides sufficient exploration without explicit cooldowns.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{fig2_ablation.pdf}
\caption{Ablation study results. Left: feature matrix showing which components are enabled. Right: solve rates for each configuration. Decay shows the largest observed effect (+10\%), followed by examples (+6.7\%), though neither reaches statistical significance at $n=30$. Inhibition shows no detectable effect.}
\label{fig:ablation}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{fig5_contributions.pdf}
\caption{Individual feature contributions to solve rate. Decay contributes +10.0\%, examples contribute +6.7\%, and inhibition shows no measurable effect in this domain.}
\label{fig:contributions}
\end{figure}

\subsubsection{Negative Pheromones}

In addition to positive pheromones (successful patches stored for few-shot examples), we implement \emph{negative pheromones}: tracking rejected patches that worsened pressure. When agents repeatedly propose ineffective patches (pressure stuck at maximum), the system accumulates rejection history and injects guidance into subsequent prompts.

Unlike the ``AVOID'' framing that small models (1.5B parameters) struggle to follow, we use \emph{positive language}: rejected empty-room patches become ``TIP: Schedule meetings in Room A (improves by X).'' This reframes what \emph{not} to do as what \emph{to try instead}.

Negative pheromones decay at the same rate as positive examples ($\text{weight} \times 0.95$ per tick, evicted below 0.1), ensuring that old failures don't permanently block valid approaches. Up to 3 recent rejections per region are included in prompts as ``Hints for better scheduling.''

\subsection{Scaling Experiments}
\label{sec:scaling}

Pressure-field maintains consistent performance from 1 to 4 agents on easy difficulty:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Agents} & \textbf{Solved/N} & \textbf{Rate} & \textbf{95\% \acs{ci}} \\
\midrule
1 & 25/30 & 83.3\% & 66.4\%--92.7\% \\
2 & 28/30 & 93.3\% & 78.7\%--98.2\% \\
4 & 25/30 & 83.3\% & 66.4\%--92.7\% \\
\bottomrule
\end{tabular}
\caption{Pressure-field scaling from 1 to 4 agents (easy difficulty, 30 trials each). Performance remains stable across agent counts.}
\label{tab:scaling}
\end{table}

The result demonstrates \emph{robustness}: pressure-field coordination maintains consistent solve rates despite 4$\times$ variation in agent count. This validates Theorem~\ref{thm:linear-scaling}: coordination overhead remains $O(1)$, enabling effective scaling. The slight peak at 2 agents (93.3\%) is within \ac{ci} overlap of 1 and 4 agents, indicating no significant agent-count effect.

\subsection{Band and Model Escalation: FM-MAS Symbiosis in Practice}
\label{sec:escalation}

The escalation mechanism demonstrates the FM-MAS symbiosis that pressure-field coordination enables. Traditional approaches face a dilemma: use a large, capable model everywhere (expensive) or use a small, efficient model everywhere (limited). Pressure-field coordination dissolves this dilemma through gradient-driven capability invocation: the coordination mechanism itself determines when greater \ac{fm} capability is needed.

Ant colonies balance exploitation of known food sources against exploration for new ones. When a pheromone trail grows stale---indicating a depleted source---foragers abandon trail-following and resume random exploration, eventually discovering new paths that become the next generation of trails. This exploitation-exploration balance is fundamental to stigmergic systems: premature commitment to suboptimal solutions must be counteracted by mechanisms that restore exploratory behavior.

Our escalation mechanism implements this principle through two complementary dynamics. \emph{Band escalation} governs the exploitation-exploration trade-off within a single model: when pressure velocity drops to zero (the ``trail goes cold''), sampling parameters shift from exploitation (low temperature, focused proposals) through balanced to exploration (high temperature, diverse proposals). This mirrors the ant's behavioral switch from trail-following to random wandering when pheromone signals weaken.

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Band} & \textbf{Temperature} & \textbf{Top-p} \\
\midrule
Exploitation & 0.15--0.35 & 0.80--0.90 \\
Balanced & 0.35--0.55 & 0.85--0.95 \\
Exploration & 0.55--0.85 & 0.90--0.98 \\
\bottomrule
\end{tabular}
\caption{Sampling parameter ranges per band. Temperature and top-p are randomly sampled within range for diversity. Escalation proceeds Exploitation $\to$ Balanced $\to$ Exploration as pressure velocity stalls.}
\label{tab:bands}
\end{table}

\emph{Model escalation} addresses a different failure mode: when exploration within a model's capability envelope fails to discover productive paths, the system recruits more capable \acp{fm}. This is where FM-MAS symbiosis becomes concrete: the \ac{mas} coordination mechanism (pressure-field) adaptively invokes higher-capability \acp{fm} based on pressure signals alone. No explicit task decomposition is needed---the pressure gradient indicates that current capabilities are insufficient, triggering escalation. Model escalation (0.5b $\to$ 1.5b $\to$ 3b) reserves greater reasoning capacity for regions that resist simpler approaches. Each model upgrade resets to exploitation band, giving the more capable model opportunity to exploit solutions invisible to its predecessor before resorting to exploration.

This architecture instantiates the bidirectional relationship between \acp{fm} and \acp{mas}:

\begin{itemize}
\item \textbf{FM contribution}: Each model tier provides broad solution coverage without requiring explicit action enumeration. The 3b model can propose patches invisible to the 0.5b model, but both operate through the same interface---observe pressure, propose patch.
\item \textbf{MAS contribution}: The pressure gradient provides the objective criterion for when to escalate. No heuristics about ``problem difficulty'' are needed; stagnant pressure velocity is sufficient signal. The coordination mechanism manages heterogeneous \ac{fm} capabilities without explicit capability models.
\end{itemize}

This two-level mechanism---behavioral adaptation within agents (band escalation) and capability escalation across agents (model escalation)---maintains the stigmergic principle: coordination emerges from environment signals (pressure gradients) rather than explicit planning. The system does not ``decide'' to explore or escalate; it reacts to pressure stagnation, just as ants react to pheromone decay. The result is adaptive \ac{fm} utilization: cheap models handle easy regions, expensive models are reserved for regions where cheap models stall.

\subsection{Difficulty Scaling}

Performance varies across difficulty levels, revealing the unique strength of pressure-field coordination:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Difficulty} & \textbf{Pressure-field} & \textbf{Conversation} & \textbf{Hierarchical} & \textbf{Sequential/Random} \\
\midrule
Easy & 86.7\% (78/90) & 33.3\% (30/90) & 4.4\% (4/90) & 1.1\% (1/90) \\
Medium & 43.3\% (39/90) & 0.0\% (0/90) & 0.0\% (0/90) & 0.0\% (0/90) \\
Hard & 15.6\% (14/90) & 0.0\% (0/90) & 0.0\% (0/90) & 0.0\% (0/90) \\
\midrule
\textbf{Total} & \textbf{48.5\%} (131/270) & \textbf{11.1\%} (30/270) & \textbf{1.5\%} (4/270) & \textbf{0.4\%} (1/270) \\
\bottomrule
\end{tabular}
\caption{Solve rate by difficulty level (90 trials each per difficulty, 270 per strategy total). Only pressure-field solves medium and hard problems. See Table~\ref{tab:main-results} for confidence intervals.}
\label{tab:difficulty}
\end{table}

The difficulty scaling reveals critical insights:

\begin{enumerate}
\item \textbf{Pressure-field is the only strategy that scales}: While all strategies degrade on harder problems, pressure-field maintains meaningful solve rates (43.3\% medium, 15.6\% hard) where all baselines achieve 0\%.

\item \textbf{The gap widens with difficulty}: On easy problems, pressure-field leads by 53.4 percentage points over conversation (86.7\% vs 33.3\%). On medium and hard problems, the gap becomes absolute---pressure-field solves problems that no baseline can solve.

\item \textbf{Effect sizes are large}: All pairwise comparisons on easy problems exceed Cohen's ``large effect'' threshold ($h > 0.8$); see Figure~\ref{fig:effect-sizes} for the full breakdown.
\end{enumerate}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{fig3_effect_sizes.pdf}
\caption{Effect sizes (Cohen's $h$) for pressure-field versus each baseline on easy problems. The dashed line indicates the ``large effect'' threshold ($h = 0.8$). All comparisons exceed this threshold, with effects ranging from $h = 1.16$ (vs conversation) to $h = 2.18$ (vs sequential/random).}
\label{fig:effect-sizes}
\end{figure}

\subsection{Convergence Speed}

For solved cases, pressure-field converges faster than baselines on easy problems and maintains consistent convergence speed across difficulty levels:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Strategy} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} \\
\midrule
Pressure-field & 17.8 (n=78) & 34.6 (n=39) & 32.3 (n=14) \\
Conversation & 29.4 (n=30) & --- & --- \\
Hierarchical & 40.0 (n=4) & --- & --- \\
\bottomrule
\end{tabular}
\caption{Average ticks to solution by difficulty (solved cases only). Only pressure-field solves medium and hard problems. Dashes indicate no solved cases.}
\label{tab:convergence}
\end{table}

On easy problems, pressure-field solves 1.65$\times$ faster than conversation and 2.2$\times$ faster than hierarchical. Notably, pressure-field's convergence speed on hard problems (32.3 ticks) is comparable to medium problems (34.6 ticks)---the hard problems that \emph{do} get solved converge at similar rates, suggesting that solvability rather than convergence speed is the limiting factor on difficult problems. This bimodal pattern---fast convergence when solvable, complete failure otherwise---suggests that model capability rather than search time is the limiting factor on hard problems.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.65\textwidth]{fig4_efficiency.pdf}
\caption{Mean ticks to solution on easy problems (solved cases only). Error bars show standard error. Pressure-field converges fastest (17.8 ticks), followed by conversation (29.4 ticks) and hierarchical (40.0 ticks).}
\label{fig:efficiency}
\end{figure}

\subsection{Final Pressure Analysis}

For both solved and unsolved cases, final pressure reveals solution quality:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Strategy} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} \\
\midrule
Pressure-field & 0.13 & 1.02 & 2.48 \\
Conversation & 26.5 & 59.3 & 92.2 \\
Hierarchical & 28.6 & 59.4 & 88.0 \\
\bottomrule
\end{tabular}
\caption{Average final pressure by difficulty (lower is better).}
\label{tab:final-pressure}
\end{table}

Pressure-field achieves 200$\times$ lower final pressure on easy, 57$\times$ lower on medium, and 35$\times$ lower on hard problems compared to conversation and hierarchical baselines. Even when pressure-field does not fully solve a problem, it achieves much better partial solutions.

\subsection{Token Efficiency}
\label{sec:token-efficiency}

A natural question is whether pressure-field's superior solve rate comes at the cost of increased \ac{llm} usage. We tracked token consumption across all strategies:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Strategy} & \textbf{Prompt} & \textbf{Completion} & \textbf{Total} \\
\midrule
Pressure-field & 532,106 & 85,165 & 617,271 \\
Conversation & 154,845 & 6,626 & 161,472 \\
Hierarchical & 32,662 & 5,424 & 38,087 \\
Sequential & 40,667 & 5,239 & 45,906 \\
Random & 41,227 & 5,253 & 46,480 \\
\bottomrule
\end{tabular}
\caption{Average token usage per trial. Pressure-field uses more tokens per trial due to parallel multi-agent execution.}
\label{tab:tokens-per-trial}
\end{table}

At first glance, pressure-field appears expensive: 617K tokens per trial versus conversation's 161K---roughly 4$\times$ higher. However, this raw comparison ignores \emph{success rate}. The relevant metric for practical deployment is \textbf{tokens per successful solve}:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Strategy} & \textbf{Total Tokens} & \textbf{Solves} & \textbf{Tokens/Solve} \\
\midrule
Pressure-field & 166.7M & 131 & 1.27M \\
Conversation & 43.6M & 30 & 1.45M \\
\bottomrule
\end{tabular}
\caption{Token efficiency per successful solve (270 trials each). Despite higher per-trial cost, pressure-field achieves lower cost per solve due to its superior success rate.}
\label{tab:tokens-per-solve}
\end{table}

Pressure-field requires 1.27M tokens per solve versus conversation's 1.45M---\textbf{12\% more efficient} per successful outcome. The apparent cost disadvantage inverts when accounting for success rate.

Token usage also varies by outcome:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{Strategy} & \textbf{Solved Trials} & \textbf{Unsolved Trials} \\
\midrule
Pressure-field & 318K (n=131) & 900K (n=139) \\
Conversation & 78K (n=30) & 172K (n=240) \\
\bottomrule
\end{tabular}
\caption{Average tokens by outcome. Both strategies use fewer tokens on solved trials (early termination), but pressure-field's unsolved trials are more expensive due to sustained parallel exploration.}
\label{tab:tokens-by-outcome}
\end{table}

Solved trials terminate early, using fewer tokens. For pressure-field, the 2.8$\times$ gap between solved (318K) and unsolved (900K) reflects the cost of exhaustive exploration when a problem proves intractable. Conversation's smaller gap (2.2$\times$) indicates less intensive search---which may explain its lower solve rate on difficult problems.

\textbf{Cost control through escalation.} The band and model escalation mechanism (Section~\ref{sec:escalation}) serves as an implicit cost-control mechanism. The system begins with cheap exploration: small models (0.5b) and exploitation-focused sampling. Tokens are spent on larger models and broader exploration \emph{only when pressure stagnates}---that is, when the problem actually requires more capable search. Easy problems that solve quickly never trigger escalation, consuming only baseline tokens. The 4$\times$ per-trial cost difference reflects the average across all difficulty levels; on easy problems where pressure-field solves 86.7\% of instances, most trials terminate before expensive escalation occurs.
